{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "residential-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents=[]\n",
    "\n",
    "# for category in movie_reviews.categories():\n",
    "#     for fileid in movie_reviews.fileids(category):\n",
    "#         documents.append((list(movie_reviews.words(fileid)),category))\n",
    "        \n",
    "        \n",
    "\n",
    "# all_words=[]\n",
    "# for w in movie_reviews.words():\n",
    "#     w=w.lower() ## all words in lower case\n",
    "#     all_words.append(w)\n",
    "    \n",
    "    \n",
    "    \n",
    "# all_words = nltk.FreqDist(all_words)   ## frequency distribution plot\n",
    "# print(all_words.most_common(15))   ## first 15 most common words\n",
    "\n",
    "# word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "# def find_features(document):\n",
    "#     words=set(document)\n",
    "#     features={}   ## created a dictionary\n",
    "#     for w in word_features:\n",
    "#         features[w]=(w in words)  ## will set the value a s 1 if there else 0\n",
    "#         ## will create a boolean value for each word\n",
    "    \n",
    "#     return features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# idtrain = feature_sets[:1800]\n",
    "# idtest = feature_sets[1800:]\n",
    "\n",
    "# basic_classifier  = nltk.NaiveBayesClassifier.train(idtrain)\n",
    "# mnb_classifier = SklearnClassifier(MultinomialNB())\n",
    "# bnb_classifier = SklearnClassifier(BernoulliNB())\n",
    "# log_classifier = SklearnClassifier(LogisticRegression(max_iter=1000))\n",
    "# sgd_classifier = SklearnClassifier(SGDClassifier())\n",
    "# svc_classifier = SklearnClassifier(SVC())\n",
    "# linear_svc_classifier = SklearnClassifier(LinearSVC())\n",
    "# nu_svc_classifier = SklearnClassifier(NuSVC(nu=0.8))\n",
    "\n",
    "\n",
    "# k=[basic_classifier,mnb_classifier,bnb_classifier,log_classifier,sgd_classifier,svc_classifier,linear_svc_classifier,nu_svc_classifier]\n",
    "\n",
    "# for i in k:\n",
    "#     i.train(idtrain)\n",
    "#     accuracy = nltk.classify.accuracy(i,idtest)\n",
    "#     print(f'accuracy with {i} is {accuracy}')\n",
    "\n",
    "    \n",
    "    \n",
    "# def mode_function(x):\n",
    "#     k=nltk.FreqDist(x)   \n",
    "#     return list(k.keys())[0]\n",
    "\n",
    "\n",
    "\n",
    "# class voteclassifier(ClassifierI):\n",
    "#     def __init__(self,*classifiers):\n",
    "#         self._classifiers = classifiers\n",
    "        \n",
    "#     def classify(self,features):\n",
    "#         votes=[]\n",
    "#         for c in self._classifiers:\n",
    "#             v = c.classify(features)\n",
    "#             votes.append(v)\n",
    "#         return (mode_function(votes))\n",
    "    \n",
    "#     def confidence(self,features):\n",
    "#         votes=[]\n",
    "#         for c in self._classifiers:\n",
    "#             v = c.classify(features)\n",
    "#             votes.append(v)\n",
    "#         choice_votes = votes.count(mode_function(votes)) ## how many time the mode value has appeared in the function\n",
    "#         conf = choice_votes / len(votes)\n",
    "#         return conf\n",
    "    \n",
    "    \n",
    "    \n",
    "# voted_classifer = voteclassifier(basic_classifier,mnb_classifier,bnb_classifier,log_classifier,sgd_classifier,svc_classifier,linear_svc_classifier,nu_svc_classifier)\n",
    "\n",
    "# print(\"voted accuracy is : \", nltk.classify.accuracy(voted_classifer,idtest))\n",
    "\n",
    "\n",
    "# ## lets check for idtest[0][0]\n",
    "# print(\"classification : \",voted_classifer.classify(idtest[0][0]), \"confidence %:\",voted_classifer.confidence(idtest[0][0]))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "equal-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets use the prvious knowledge on performing\n",
    "## twitter sentiment analysis\n",
    "\n",
    "## this is the link for the data\n",
    "## https://pythonprogramming.net/static/downloads/short_reviews/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "municipal-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_function(x):\n",
    "    k=nltk.FreqDist(x)   \n",
    "    return list(k.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "detailed-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB ## when its not a binary distribution\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC ## number of support vectors : NuSVC\n",
    "\n",
    "\n",
    "class voteclassifier(ClassifierI):\n",
    "    def __init__(self,*classifiers):\n",
    "        self._classifiers = classifiers\n",
    "        \n",
    "    def classify(self,features):\n",
    "        votes=[]\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return (mode_function(votes))\n",
    "    \n",
    "    def confidence(self,features):\n",
    "        votes=[]\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        choice_votes = votes.count(mode_function(votes)) ## how many time the mode value has appeared in the function\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "alien-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = open(\"C:\\\\Users\\\\Shankii\\\\Desktop\\\\kaglle\\\\Twitter\\\\positive.txt\",\"r\").read()\n",
    "negative = open(\"C:\\\\Users\\\\Shankii\\\\Desktop\\\\kaglle\\\\Twitter\\\\negative.txt\",'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dutch-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = []\n",
    "# for r in positive.split(\"\\n\"): ## split by line\n",
    "#     documents.append((r,\"pos\")) ## review and its type stored in document as pos\n",
    "\n",
    "\n",
    "# for r in negative.split(\"\\n\"): ## split by line\n",
    "#     documents.append((r,\"neg\")) ## review and its type stored in document as neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-logging",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "small-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# all_words=[]\n",
    "# pos_words = word_tokenize(positive)\n",
    "# neg_words = word_tokenize(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-schedule",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "technological-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for w in pos_words:\n",
    "#     all_words.append(w.lower()) \n",
    "\n",
    "# for w in neg_words:\n",
    "#     all_words.append(w.lower())   ## appended words from neg reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-poison",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "everyday-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words=[]\n",
    "documents=[]\n",
    "\n",
    "## sinnce from part of tag speech\n",
    "## we know j: adjective,\n",
    "## r is adverb\n",
    "## v is verb\n",
    "\n",
    "## lets just take only adjective words\n",
    "\n",
    "\n",
    "allowed_words_type=[\"J\"]\n",
    "\n",
    "for p in positive.split(\"\\n\"):  ## all the words till new line is detected\n",
    "    documents.append((p,\"pos\"))\n",
    "    words = word_tokenize(p)\n",
    "    pos = nltk.pos_tag(words)\n",
    "    \n",
    "    ## w[1][0] is the first letter [0] of each word\n",
    "    \n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_words_type:   ## first letter of each word if J : Adjective\n",
    "            all_words.append(w[0].lower())\n",
    "            \n",
    "            \n",
    "            \n",
    "for p in negative.split(\"\\n\"):  ## all the words till new line is detected\n",
    "    documents.append((p,\"neg\"))\n",
    "    words = word_tokenize(p)\n",
    "    pos = nltk.pos_tag(words)\n",
    "    \n",
    "    ## w[1][0] is the first letter [0] of each word\n",
    "    \n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_words_type:   ## first letter of each word if J : Adjective\n",
    "            all_words.append(w[0].lower())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "chinese-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets pickle the documnets\n",
    "## so that when you run the data\n",
    "## the time required is shortened\n",
    "import pickle\n",
    "save_documents = open(\"documents_twitter.pickle\",\"wb\")\n",
    "pickle.dump(documents,save_documents)\n",
    "save_documents.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-russell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "august-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dietary-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(all_words.keys())[:5000]\n",
    "\n",
    "## you can change the number of valuesu wnna take as samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "heated-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets pickle word_fetures\n",
    "\n",
    "save_word_features = open(\"word_features.pickle\",\"wb\")\n",
    "pickle.dump(word_features,save_word_features)\n",
    "save_word_features.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bizarre-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "        # words=set(document)  ## you cannot use this, since the input is now sring and not words as seperate\n",
    "        # but the above will work but with less accurcay\n",
    "        \n",
    "    words = word_tokenize(document)\n",
    "    features={}   ## created a dictionary\n",
    "    for w in word_features:\n",
    "        features[w]=(w in words)  ## will set the value a s 1 if there else 0\n",
    "        ## will create a boolean value for each word\n",
    "    \n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "thick-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev),category) for (rev,category) in documents]\n",
    "\n",
    "\n",
    "\n",
    "save_featuresets = open(\"featuresets_twitter.pickle\", \"wb\")\n",
    "pickle.dump(featuresets,save_featuresets)\n",
    "save_featuresets.close()\n",
    "\n",
    "import random\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eight-quilt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10664"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-delhi",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "plain-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB ## when its not a binary distribution\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC ## number of support vectors : NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "transparent-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## since length is 10000\n",
    "\n",
    "# idtrain = featuresets[:9000]\n",
    "# idtest = featuresets[9000:]\n",
    "\n",
    "# basic_classifier  = nltk.NaiveBayesClassifier.train(idtrain)\n",
    "# mnb_classifier = SklearnClassifier(MultinomialNB())\n",
    "# bnb_classifier = SklearnClassifier(BernoulliNB())\n",
    "# log_classifier = SklearnClassifier(LogisticRegression(max_iter=1000))\n",
    "# sgd_classifier = SklearnClassifier(SGDClassifier())\n",
    "# svc_classifier = SklearnClassifier(SVC())\n",
    "# linear_svc_classifier = SklearnClassifier(LinearSVC())\n",
    "# nu_svc_classifier = SklearnClassifier(NuSVC(nu=0.8))\n",
    "\n",
    "\n",
    "\n",
    "# k=[basic_classifier,mnb_classifier,bnb_classifier,log_classifier,sgd_classifier,svc_classifier,linear_svc_classifier,nu_svc_classifier]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "brutal-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in k:\n",
    "#     i.train(idtrain)\n",
    "#     accuracy = nltk.classify.accuracy(i,idtest)\n",
    "#     print(f'accuracy with {i} is {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "nuclear-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "## since length is 10000\n",
    "\n",
    "training_set = featuresets[:9000]\n",
    "testing_set = featuresets[9000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "manufactured-photograph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 72.89663461538461\n",
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     18.0 : 1.0\n",
      "                  stupid = True              neg : pos    =     17.4 : 1.0\n",
      "                powerful = True              pos : neg    =     16.1 : 1.0\n",
      "                 generic = True              neg : pos    =     15.4 : 1.0\n",
      "                mediocre = True              neg : pos    =     14.1 : 1.0\n",
      "                haunting = True              pos : neg    =     13.2 : 1.0\n",
      "               inventive = True              pos : neg    =     13.2 : 1.0\n",
      "              refreshing = True              pos : neg    =     13.2 : 1.0\n",
      "              unexpected = True              pos : neg    =     13.2 : 1.0\n",
      "                    imax = True              pos : neg    =     12.5 : 1.0\n",
      "                  unique = True              pos : neg    =     12.5 : 1.0\n",
      "                 routine = True              neg : pos    =     12.1 : 1.0\n",
      "                  boring = True              neg : pos    =     12.0 : 1.0\n",
      "                    flat = True              neg : pos    =     11.6 : 1.0\n",
      "                    warm = True              pos : neg    =     11.6 : 1.0\n",
      "MNB_classifier accuracy percent: 73.13701923076923\n",
      "BernoulliNB_classifier accuracy percent: 73.13701923076923\n",
      "LogisticRegression_classifier accuracy percent: 73.3173076923077\n",
      "LinearSVC_classifier accuracy percent: 71.875\n",
      "SGDClassifier accuracy percent: 71.875\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pickled_algos/SGDC_classifier5k.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-f5ba44bcd998>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SGDClassifier accuracy percent:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSGDC_classifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0msave_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pickled_algos/SGDC_classifier5k.pickle\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSGDC_classifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_classifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[0msave_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pickled_algos/SGDC_classifier5k.pickle'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "save_classifier = open(\"originalnaivebayes_twitter.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "#=============================================================================\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "save_classifier = open(\"MNB_classifier_twitter.pickle\",\"wb\")\n",
    "pickle.dump(MNB_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "#=============================================================================\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "save_classifier = open(\"BernoulliNB_classifier_twitter.pickle\",\"wb\")\n",
    "pickle.dump(BernoulliNB_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "#=============================================================================\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "save_classifier = open(\"LogisticRegression_classifier_twitter.pickle\",\"wb\")\n",
    "pickle.dump(LogisticRegression_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "#=============================================================================\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "save_classifier = open(\"LinearSVC_classifier_twitter.pickle\",\"wb\")\n",
    "pickle.dump(LinearSVC_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "#=============================================================================\n",
    "## taking too much time to run\n",
    "\n",
    "# NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "# NuSVC_classifier.train(training_set)\n",
    "# print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)\n",
    "\n",
    "# save_classifier = open(\"NuSVC_classifier_twitter.pickle\",\"wb\")\n",
    "# pickle.dump(NuSVC_classifier, save_classifier)\n",
    "# save_classifier.close()\n",
    "\n",
    "#==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "mighty-childhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier accuracy percent: 71.9951923076923\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SGDC_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDC_classifier.train(training_set)\n",
    "print(\"SGDClassifier accuracy percent:\",nltk.classify.accuracy(SGDC_classifier, testing_set)*100)\n",
    "\n",
    "save_classifier = open(\"SGDC_classifier_twitter.pickle\",\"wb\")\n",
    "pickle.dump(SGDC_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-situation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "considerable-scope",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Shankii\\\\Documents\\\\NLTK'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-findings",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-foundation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-morocco",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
